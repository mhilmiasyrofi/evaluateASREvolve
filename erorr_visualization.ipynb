{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Visualization\n",
    "\n",
    "- Quantifying Errors\n",
    "\t- From Determined Test Cases,  \n",
    "For each word, measure the number of *correctly* and *incorrectly* interpreted\n",
    "- Classifying Errors\n",
    "\t- Miss interpretation of the words  \n",
    "For each word, log the transcription, what is the the misinterpretation of the error  \n",
    "â†’ calculate the occurence  \n",
    "â†’ most common error for a given word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "import numpy\n",
    "import glob\n",
    "import jiwer\n",
    "import collections\n",
    "import helper\n",
    "\n",
    "# TODO: remove warning, put text preprocessing as helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reference = []\n",
    "        self.transcription = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        return helper.preprocess_text(text)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_reference(self):\n",
    "        return self.reference\n",
    "\n",
    "    def get_transcription(self):\n",
    "        return self.transcription\n",
    "\n",
    "    def add_reference(self, reference):\n",
    "        self.reference.append(reference)\n",
    "    \n",
    "    def add_transcription(self, transcription):\n",
    "        self.transcription.append(transcription)\n",
    "\n",
    "    def add_reference_transcription(self, reference, transcription):\n",
    "        self.add_reference(reference)\n",
    "        self.add_transcription(transcription)\n",
    "    \n",
    "    def length(self):\n",
    "        assert len(self.reference) == len(self.transcription)\n",
    "        return len(self.reference)\n",
    "\n",
    "    def print_reference_transcription(self, i):\n",
    "        if i >= 0 and i < len(self.reference):\n",
    "            print(\"Reference:   \\t: \", self.reference[i])\n",
    "            print(\"Transcription: \\t: \", self.transcription[i])\n",
    "\n",
    "    \n",
    "    def print_head(self):\n",
    "        self.print_reference_transcription(i=0)\n",
    "\n",
    "    def print_tail(self):\n",
    "        self.print_reference_transcription(i=self.length()-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBRISPEECH_TYPES = [\"test-clean\",\n",
    "                    \"test-other\",\n",
    "                    \"dev-clean\",\n",
    "                    \"dev-other\"]\n",
    "\n",
    "# LIBRISPEECH_TYPES = [\"test-clean\"]\n",
    "\n",
    "# LIBRISPEECH_TYPES = [\"test-other\"]\n",
    "\n",
    "# LIBRISPEECH_TYPES = [\"dev-clean\"]\n",
    "\n",
    "# LIBRISPEECH_TYPES = [\"dev-other\"]\n",
    "\n",
    "\n",
    "LIBRISPEECH_DIRS = []\n",
    "\n",
    "for type in LIBRISPEECH_TYPES :\n",
    "    LIBRISPEECH_DIRS.append(f\"LibriSpeech/{type}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_librispeech_data():\n",
    "    data = Data(\"librispeech\")\n",
    "    model_dir = \"deepspeech\"\n",
    "\n",
    "    \n",
    "    for librispeech_dir in LIBRISPEECH_DIRS:\n",
    "        # librispeech_dir = \"LibriSpeech/test-clean/\"\n",
    "\n",
    "        for filename in glob.iglob(librispeech_dir + '**/*.trans.txt', recursive=True):\n",
    "            \n",
    "            file = open(filename)\n",
    "\n",
    "            for line in file.readlines():\n",
    "                idx = line.split()[0]\n",
    "                reference_text = \" \".join(line.split()[1:])\n",
    "                \n",
    "\n",
    "                fid = \"/\".join(idx.split(\"-\")[:-1]) # idx to file id\n",
    "\n",
    "                fname = os.path.join(librispeech_dir, fid, idx)\n",
    "                transcription_path = fname + \".\" + model_dir + \".transcription.txt\"\n",
    "                if os.path.exists(transcription_path):\n",
    "                    transcription = helper.read_transcription(transcription_path)\n",
    "                    \n",
    "                    # preprocess the text and transcription\n",
    "                    reference_text = helper.preprocess_text(reference_text)\n",
    "                    transcription = helper.preprocess_text(transcription)\n",
    "                    \n",
    "                    data.add_reference_transcription(reference_text, transcription)\n",
    "                else:\n",
    "                    raise ValueError(\"missing transcription: \" + transcription_path)\n",
    "\n",
    "            file.close()\n",
    "        \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  eleven oclock had struck it was a fine clear night there were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in the valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  now i wouldnt have felt that way\n",
      "Transcription: \t:  now i wouldnt have felt that way\n"
     ]
    }
   ],
   "source": [
    "data = read_librispeech_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_fpath: str):\n",
    "    file = open(corpus_fpath)\n",
    "    corpus = file.readlines()\n",
    "    texts = []\n",
    "    for text in corpus:\n",
    "        texts.append(text[:-1])\n",
    "\n",
    "    return texts\n",
    "\n",
    "def read_crossasr_data():\n",
    "    data = Data(\"crossasr\")\n",
    "\n",
    "    # mode = \"europarl\"\n",
    "    mode = \"librispeech\"\n",
    "\n",
    "    corpus_fpaths = []\n",
    "    transcription_dirs = []\n",
    "\n",
    "\n",
    "    if mode == \"europarl\":\n",
    "        tts_name = \"rv\"\n",
    "    elif mode == \"librispeech\":\n",
    "        tts_name = \"google\"\n",
    "\n",
    "    asr_name = \"deepspeech\"\n",
    "    \n",
    "    if mode == \"europarl\" :\n",
    "        \n",
    "        corpus_fpath = \"CrossASR/europarl-seed2021/corpus/europarl-20000.txt\"\n",
    "        transcription_dir = \"CrossASR/europarl-seed2021/data/transcription\"\n",
    "        \n",
    "        corpus_fpaths.append(corpus_fpath)\n",
    "        transcription_dirs.append(transcription_dir)\n",
    "        \n",
    "    elif mode == \"librispeech\" :\n",
    "\n",
    "        for t in LIBRISPEECH_TYPES:\n",
    "        \n",
    "            corpus_fpath = f\"CrossASR/librispeech-crossasr-{t}/corpus/librispeech-{t}-corpus.txt\"\n",
    "            transcription_dir = f\"CrossASR/librispeech-crossasr-{t}/data/transcription\"\n",
    "            \n",
    "            corpus_fpaths.append(corpus_fpath)\n",
    "            transcription_dirs.append(transcription_dir)\n",
    "\n",
    "\n",
    "\n",
    "    for corpus_fpath, transcription_dir in zip(corpus_fpaths, transcription_dirs):\n",
    "    \n",
    "        transcription_dir = os.path.join(transcription_dir, tts_name, asr_name)\n",
    "\n",
    "        references = read_corpus(corpus_fpath)\n",
    "        \n",
    "        for i in range(len(references)):\n",
    "            if mode == \"europarl\" :\n",
    "                transcription_path = os.path.join(transcription_dir, f\"{i+1}.txt\")\n",
    "            elif mode == \"librispeech\" :\n",
    "                transcription_path = os.path.join(transcription_dir, f\"{i}.txt\")\n",
    "            transcription = helper.read_transcription(transcription_path)\n",
    "            transcription = helper.preprocess_text(transcription)\n",
    "\n",
    "            data.add_reference_transcription(references[i], transcription)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  oclock had struck it was a fine clear night they were the only persons on the road and day sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  now i wouldnt have felt that way\n",
      "Transcription: \t:  now i wouldnt have felt that way\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_evaluation.asr_evaluation import asr_evaluation\n",
    "\n",
    "class Analyzer(object):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## used for caching\n",
    "        self.infos = {}\n",
    "        self.word_count = {}\n",
    "        self.word_accuracy = {}\n",
    "        self.word_accuracy_with_count = {}\n",
    "        self.common_errors = {}\n",
    "\n",
    "        \n",
    "    def analyze(self, data: Data):\n",
    "\n",
    "        ## get from cache if it is already computed before\n",
    "        if data.get_name() in self.infos :\n",
    "            return self.infos[data.get_name()], self.word_count[data.get_name()]\n",
    "        \n",
    "        infos = []\n",
    "        word_count = collections.Counter()\n",
    "        \n",
    "        for reference, transcription, in zip(data.get_reference(), data.get_transcription()) :\n",
    "            \n",
    "            ## create statistics for word counter\n",
    "            word_count += collections.Counter(reference.split())\n",
    "            \n",
    "            ## create statistics for errors\n",
    "            wer = jiwer.wer(reference, transcription)\n",
    "            if wer != 0:\n",
    "                evaluation = asr_evaluation.ASREvaluation()\n",
    "                evaluation.detect_word_error(reference, transcription)\n",
    "                confusion = evaluation.get_confusions()\n",
    "                infos.append(\n",
    "                    {\"confusion\": confusion, \"reference\": reference, \"transcription\": transcription})\n",
    "        \n",
    "        ## update the cache\n",
    "        self.infos[data.get_name()] = infos\n",
    "        self.word_count[data.get_name()] = word_count\n",
    "        \n",
    "        \n",
    "        return infos, word_count\n",
    "\n",
    "    def calculate_word_accuracy(self, data: Data):\n",
    "        \"\"\"Calculate word accuracy, which is the number of error (deletion or subsitution) divided by the number of word count\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## use caching if it is already computed before\n",
    "        if data.get_name() in self.word_accuracy:\n",
    "            return self.word_accuracy[data.get_name()]\n",
    "\n",
    "        \n",
    "        infos, word_count = self.analyze(data)\n",
    "        error_count = collections.Counter()\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "\n",
    "            ## get error from word substitution\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                \n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "                \n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "            ## get error from word deletion\n",
    "            if len(confusion[\"deletion\"]) > 0:\n",
    "\n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"deletion\"])):\n",
    "                    word_reference = confusion[\"deletion\"][i][\"word\"]\n",
    "                    count = confusion[\"deletion\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "\n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "        \n",
    "        word_accuracy = {}\n",
    "        word_accuracy_with_count = {}\n",
    "        for word in word_count :\n",
    "            \n",
    "            ## if the word in the reference_text does not appear in the error word\n",
    "            ## then all the occurence of the word is correctly predicted \n",
    "            if word not in error_count :\n",
    "                word_accuracy[word] = 100.0 \n",
    "                word_accuracy_with_count[word] = [100.0 , word_count[word]]\n",
    "            else :\n",
    "                curr_word_accuracy = 100.0 - (100 * round(error_count[word]/word_count[word], 2))\n",
    "                assert curr_word_accuracy <= 100 and curr_word_accuracy >= 0\n",
    "                word_accuracy[word] = curr_word_accuracy\n",
    "                word_accuracy_with_count[word] = [curr_word_accuracy, word_count[word]]\n",
    "\n",
    "        ## sort the word accuracy based on the word_accuracy\n",
    "        word_accuracy = dict(sorted(word_accuracy.items(),\n",
    "                                    key=lambda item: (item[1], item[0]), reverse=True))\n",
    "\n",
    "        ## sort the word accuracy with count based on the word_accuracy\n",
    "        word_accuracy_with_count = dict(sorted(word_accuracy_with_count.items(),\n",
    "                                               key=lambda item: (item[1][0], -item[1][1], item[0]), reverse=False))\n",
    "\n",
    "        \n",
    "        ## update cache\n",
    "        self.word_accuracy[data.get_name()] = word_accuracy\n",
    "        self.word_accuracy_with_count[data.get_name()] = word_accuracy_with_count\n",
    "        \n",
    "        return word_accuracy\n",
    "\n",
    "    def get_word_accuracy(self, dataset_name:str):\n",
    "        if dataset_name in self.word_accuracy :\n",
    "            return self.word_accuracy[dataset_name]\n",
    "        return None\n",
    "\n",
    "    def get_word_accuracy_with_count(self, dataset_name: str):\n",
    "        if dataset_name in self.word_accuracy_with_count:\n",
    "            return self.word_accuracy_with_count[dataset_name]\n",
    "        return None\n",
    "\n",
    "\n",
    "    def print_word_accuracy_with_minimum_count(self, data: Data, limit=0, minimium_count=10, ascending=True):\n",
    "        \n",
    "        if not data.get_name() in self.word_accuracy_with_count :\n",
    "            self.calculate_word_accuracy(data)\n",
    "        \n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[data.get_name()]\n",
    "    \n",
    "        keys = list(word_accuracy_with_count.keys())\n",
    "        values = list(word_accuracy_with_count.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'} \\tCount\")\n",
    "        \n",
    "        j = 0\n",
    "        for i in reversed(range(len(keys))) if ascending else range(len(keys)):\n",
    "            if j < limit :\n",
    "                if values[i][1] >= minimium_count :\n",
    "                    print(f\"{keys[i]:15s} {values[i][0]:} \\t\\t{values[i][1]:}\")\n",
    "                    j += 1\n",
    "\n",
    "\n",
    "    def save_word_accuracy(self, dataset_name, fpath):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy.keys())\n",
    "            values = list(word_accuracy.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'}\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(f\"{keys[i]:15s} \\t{values[i]}\\n\")\n",
    "\n",
    "    def save_word_accuracy_with_count(self, dataset_name, fpath):\n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy_with_count.keys())\n",
    "            values = list(word_accuracy_with_count.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'} \\tCount\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(\n",
    "                    f\"{keys[i]:15s} \\t{values[i][0]} \\t{values[i][1]}\\n\")\n",
    "\n",
    "    def print_lowest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(len(keys)-limit, len(keys)):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "    def print_highest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(limit):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "\n",
    "    def get_most_common_errors(self, data: Data):\n",
    "        \n",
    "        \n",
    "        ## geta result from caching if it is already computed before\n",
    "        if data.get_name() in self.common_errors:\n",
    "            return self.common_errors[data.get_name()]\n",
    "\n",
    "\n",
    "        infos, _ = self.analyze(data)\n",
    "\n",
    "        ## TODO: use Counter library\n",
    "        common_errors = {}\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    word_substitution = confusion[\"substitution\"][i][\"word_substitution\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    if word_reference == \"and\" and word_substitution == \"terrified\":\n",
    "                        print(info[\"reference\"])\n",
    "                        print(info[\"transcription\"])\n",
    "\n",
    "\n",
    "                    if word_reference in common_errors:\n",
    "                        substitutions = common_errors[word_reference]\n",
    "                        if word_substitution in substitutions:\n",
    "                            common_errors[word_reference][word_substitution] = count + \\\n",
    "                                common_errors[word_reference][word_substitution]\n",
    "                        else:\n",
    "                            common_errors[word_reference][word_substitution] = count\n",
    "                    else:\n",
    "                        common_errors[word_reference] = {\n",
    "                            word_substitution: count}\n",
    "\n",
    "        ## sort things inside the common error\n",
    "        for key in common_errors.keys():\n",
    "            common_errors[key] = dict(sorted(common_errors[key].items(),\n",
    "                                            key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        ## sort words based on the highest occurence\n",
    "        common_errors = dict(sorted(common_errors.items(),\n",
    "                                        key=lambda item: list(item[1].values())[0], reverse=True))\n",
    "        \n",
    "        self.common_errors[data.get_name()] = common_errors\n",
    "        \n",
    "        return common_errors\n",
    "\n",
    "    def print_common_error(self, common_errors, limit=2):\n",
    "        count = 0\n",
    "        print_limit = 16\n",
    "        for word, common in common_errors.items():\n",
    "            if count < print_limit :\n",
    "                print(\"Word: \", word)\n",
    "                # print(\"Substituion: \")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(min(limit, len(keys))):\n",
    "                    print(f\"\\t{keys[i]:10s} count: {values[i]}\")\n",
    "            count += 1\n",
    "\n",
    "    def save_common_errors(self, common_errors, fpath):\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            for word, common in common_errors.items():\n",
    "                file.write(f\"Word: {word}\\n\")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(len(keys)):\n",
    "                    file.write(f\"\\t{keys[i]:10s} count: {values[i]}\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "abolitionists   0.0\n",
      "abolish         0.0\n",
      "abjured         0.0\n",
      "abilities       0.0\n",
      "abdera          0.0\n",
      "abbreviations   0.0\n",
      "abbess          0.0\n",
      "abbe            0.0\n",
      "abaft           0.0\n",
      "aaron           0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zoology         100.0\n",
      "zoologist       100.0\n",
      "zodiac          100.0\n",
      "zepplin         100.0\n",
      "yorkshire       100.0\n",
      "yesterdays      100.0\n",
      "yelling         100.0\n",
      "yarn            100.0\n",
      "yankees         100.0\n",
      "yachtsman       100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer()\n",
    "\n",
    "data = read_librispeech_data()\n",
    "\n",
    "analyzer.calculate_word_accuracy(data)\n",
    "word_accuracy = analyzer.get_word_accuracy(data.get_name())\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "professor       100.0 \t\t10\n",
      "president       100.0 \t\t10\n",
      "occasionally    100.0 \t\t10\n",
      "feast           100.0 \t\t10\n",
      "express         100.0 \t\t10\n",
      "carrying        100.0 \t\t10\n",
      "begins          100.0 \t\t10\n",
      "powder          100.0 \t\t11\n",
      "national        100.0 \t\t11\n",
      "managed         100.0 \t\t11\n",
      "hester          100.0 \t\t11\n",
      "ball            100.0 \t\t11\n",
      "mystery         100.0 \t\t12\n",
      "mentioned       100.0 \t\t12\n",
      "images          100.0 \t\t12\n",
      "charlotte       100.0 \t\t12\n",
      "regiment        100.0 \t\t13\n",
      "appointed       100.0 \t\t13\n",
      "wandering       100.0 \t\t14\n",
      "flesh           100.0 \t\t14\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  in\n",
      "\tand        count: 131\n",
      "\tto         count: 7\n",
      "Word:  a\n",
      "\tthe        count: 111\n",
      "\tof         count: 10\n",
      "Word:  and\n",
      "\tin         count: 96\n",
      "\tan         count: 16\n",
      "Word:  the\n",
      "\ta          count: 78\n",
      "\tto         count: 11\n",
      "Word:  this\n",
      "\tthe        count: 67\n",
      "\this        count: 5\n",
      "Word:  too\n",
      "\tto         count: 33\n",
      "\ttwo        count: 5\n",
      "Word:  that\n",
      "\tthe        count: 26\n",
      "\tat         count: 10\n",
      "Word:  an\n",
      "\tand        count: 25\n",
      "\tthe        count: 6\n",
      "Word:  is\n",
      "\tas         count: 21\n",
      "\twas        count: 15\n",
      "Word:  their\n",
      "\tthe        count: 19\n",
      "\ther        count: 7\n",
      "Word:  two\n",
      "\tto         count: 19\n",
      "\tlotto      count: 1\n",
      "Word:  of\n",
      "\ta          count: 19\n",
      "\tin         count: 7\n",
      "Word:  o\n",
      "\toh         count: 18\n",
      "\tof         count: 6\n",
      "Word:  it\n",
      "\ti          count: 17\n",
      "\the         count: 10\n",
      "Word:  at\n",
      "\tand        count: 16\n",
      "\ta          count: 8\n",
      "Word:  you\n",
      "\the         count: 16\n",
      "\ti          count: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/librispeech/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "abyssinians     0.0\n",
      "absorb          0.0\n",
      "aboriginal      0.0\n",
      "abner           0.0\n",
      "abed            0.0\n",
      "abdera          0.0\n",
      "abbess          0.0\n",
      "abalone         0.0\n",
      "abaft           0.0\n",
      "aaron           0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zoology         100.0\n",
      "zoologist       100.0\n",
      "zodiac          100.0\n",
      "zest            100.0\n",
      "zealand         100.0\n",
      "zeal            100.0\n",
      "youthful        100.0\n",
      "youngest        100.0\n",
      "yorkshire       100.0\n",
      "york            100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "\n",
    "# analyzer = Analyzer()\n",
    "analyzer.calculate_word_accuracy(data)\n",
    "word_accuracy = analyzer.get_word_accuracy(data.get_name())\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  and\n",
      "\tin         count: 159\n",
      "\ton         count: 6\n",
      "Word:  as\n",
      "\tis         count: 140\n",
      "\this        count: 20\n",
      "Word:  the\n",
      "\tthat       count: 64\n",
      "\tthere      count: 25\n",
      "Word:  was\n",
      "\twith       count: 56\n",
      "\tas         count: 16\n",
      "Word:  them\n",
      "\thim        count: 55\n",
      "\tthe        count: 10\n",
      "Word:  who\n",
      "\the         count: 53\n",
      "\tyou        count: 15\n",
      "Word:  a\n",
      "\tthe        count: 50\n",
      "\tand        count: 21\n",
      "Word:  are\n",
      "\ta          count: 48\n",
      "\tand        count: 29\n",
      "Word:  too\n",
      "\tto         count: 41\n",
      "\ttwo        count: 17\n",
      "Word:  im\n",
      "\tin         count: 38\n",
      "\thim        count: 11\n",
      "Word:  at\n",
      "\tit         count: 36\n",
      "\tthat       count: 7\n",
      "Word:  you\n",
      "\the         count: 35\n",
      "\ti          count: 20\n",
      "Word:  their\n",
      "\tthe        count: 33\n",
      "\tthere      count: 9\n",
      "Word:  our\n",
      "\ta          count: 30\n",
      "\tare        count: 4\n",
      "Word:  an\n",
      "\tin         count: 28\n",
      "\tand        count: 23\n",
      "Word:  thee\n",
      "\tthe        count: 27\n",
      "\the         count: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/crossasr/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "wearing         100.0 \t\t10\n",
      "visitor         100.0 \t\t10\n",
      "violin          100.0 \t\t10\n",
      "van             100.0 \t\t10\n",
      "valley          100.0 \t\t10\n",
      "union           100.0 \t\t10\n",
      "uncertain       100.0 \t\t10\n",
      "twelve          100.0 \t\t10\n",
      "tower           100.0 \t\t10\n",
      "thousands       100.0 \t\t10\n",
      "tail            100.0 \t\t10\n",
      "strain          100.0 \t\t10\n",
      "signal          100.0 \t\t10\n",
      "seriously       100.0 \t\t10\n",
      "sees            100.0 \t\t10\n",
      "sacrifice       100.0 \t\t10\n",
      "remarkable      100.0 \t\t10\n",
      "religious       100.0 \t\t10\n",
      "related         100.0 \t\t10\n",
      "reflection      100.0 \t\t10\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(\n",
    "    data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight \n",
    "\n",
    "It seems hard to compare the words intersection one-by-one. \n",
    "We will try qeurying the statistic for each word that has much error in CrossASR\n",
    "Then get corresponding statistic from Librispeech data\n",
    "\n",
    "### Combining Word Accuracy from the Two Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_data = read_crossasr_data()   ## crossasr dataq\n",
    "ls_data = read_librispeech_data()   ## librispeech data\n",
    "\n",
    "analyzer = Analyzer()\n",
    "\n",
    "analyzer.calculate_word_accuracy(cr_data)\n",
    "analyzer.calculate_word_accuracy(ls_data)\n",
    "\n",
    "\n",
    "cr_word_acc = analyzer.get_word_accuracy_with_count(cr_data.get_name())\n",
    "ls_word_acc = analyzer.get_word_accuracy_with_count(ls_data.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each error in librispeech word accuracy, inform the corresponding error on the crossasr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ls_word_acc</th>\n",
       "      <th>cr_word_acc</th>\n",
       "      <th>ls_word_count</th>\n",
       "      <th>cr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lelechka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fauchelevent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dickie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leslie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serafima</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17077</th>\n",
       "      <td>yesterdays</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17078</th>\n",
       "      <td>zepplin</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17079</th>\n",
       "      <td>zodiac</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17080</th>\n",
       "      <td>zoologist</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17081</th>\n",
       "      <td>zoology</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17082 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  ls_word_acc  cr_word_acc ls_word_count cr_word_count\n",
       "0          lelechka          0.0          0.0            35            35\n",
       "1      fauchelevent          0.0          0.0            24            24\n",
       "2            dickie          0.0          4.0            23            23\n",
       "3            leslie          0.0         91.0            23            23\n",
       "4          serafima          0.0          0.0            23            23\n",
       "...             ...          ...          ...           ...           ...\n",
       "17077    yesterdays        100.0        100.0             1             1\n",
       "17078       zepplin        100.0          0.0             1             1\n",
       "17079        zodiac        100.0        100.0             1             1\n",
       "17080     zoologist        100.0        100.0             1             1\n",
       "17081       zoology        100.0        100.0             1             1\n",
       "\n",
       "[17082 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"word\", \"ls_word_acc\",\n",
    "                  \"cr_word_acc\", \"ls_word_count\",  \"cr_word_count\"])\n",
    "for word in ls_word_acc :\n",
    "    if word in cr_word_acc :\n",
    "        df = df.append({\"word\": word,\n",
    "                        \"ls_word_acc\": ls_word_acc[word][0],\n",
    "                        \"cr_word_acc\": cr_word_acc[word][0],\n",
    "                        \"ls_word_count\": ls_word_acc[word][1],\n",
    "                        \"cr_word_count\": cr_word_acc[word][1]\n",
    "                        }, ignore_index=True)\n",
    "    else :\n",
    "        df = df.append({\"word\": word,\n",
    "                        \"ls_word_acc\": ls_word_acc[word][0], \n",
    "                        \"cr_word_acc\" : -1,\n",
    "                        \"ls_word_count\": ls_word_acc[word][1],\n",
    "                        \"cr_word_count\": -1\n",
    "                   }, ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ls_word_acc</th>\n",
       "      <th>cr_word_acc</th>\n",
       "      <th>ls_word_count</th>\n",
       "      <th>cr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word, ls_word_acc, cr_word_acc, ls_word_count, cr_word_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the row to ensure ls_word_count == cr_word_count\n",
    "df[df.apply(lambda x: x.ls_word_count != x.cr_word_count, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/combined_word_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=108454604.5, pvalue=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "# scipy.stats.pearsonr(df[\"ls_word_acc\"], df[\"cr_word_acc\"])\n",
    "scipy.stats.mannwhitneyu(df[\"ls_word_acc\"], df[\"cr_word_acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_interval</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-100</td>\n",
       "      <td>0.473850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-50</td>\n",
       "      <td>0.120897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50-100</td>\n",
       "      <td>0.279127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-25</td>\n",
       "      <td>0.009549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-50</td>\n",
       "      <td>0.072285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50-75</td>\n",
       "      <td>0.084543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75-100</td>\n",
       "      <td>0.194047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0-20</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20-40</td>\n",
       "      <td>0.072979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40-60</td>\n",
       "      <td>0.026239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60-80</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>80-100</td>\n",
       "      <td>0.174383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0-10</td>\n",
       "      <td>0.014206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10-20</td>\n",
       "      <td>-0.024035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20-30</td>\n",
       "      <td>0.094378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30-40</td>\n",
       "      <td>0.088620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40-50</td>\n",
       "      <td>0.075888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50-60</td>\n",
       "      <td>-0.016862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60-70</td>\n",
       "      <td>0.038509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>70-80</td>\n",
       "      <td>0.038901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>80-90</td>\n",
       "      <td>0.101966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>90-100</td>\n",
       "      <td>0.119379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_interval  pearson_corr\n",
       "0         0-100      0.473850\n",
       "1          0-50      0.120897\n",
       "2        50-100      0.279127\n",
       "3          0-25      0.009549\n",
       "4         25-50      0.072285\n",
       "5         50-75      0.084543\n",
       "6        75-100      0.194047\n",
       "7          0-20      0.002272\n",
       "8         20-40      0.072979\n",
       "9         40-60      0.026239\n",
       "10        60-80      0.105500\n",
       "11       80-100      0.174383\n",
       "12         0-10      0.014206\n",
       "13        10-20     -0.024035\n",
       "14        20-30      0.094378\n",
       "15        30-40      0.088620\n",
       "16        40-50      0.075888\n",
       "17        50-60     -0.016862\n",
       "18        60-70      0.038509\n",
       "19        70-80      0.038901\n",
       "20        80-90      0.101966\n",
       "21       90-100      0.119379"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stat = pd.DataFrame(columns=[\"bin_interval\", \"pearson_corr\"]) \n",
    "\n",
    "\n",
    "n_bins = [1, 2, 4, 5, 10]\n",
    "\n",
    "for n_bin in n_bins :\n",
    "    bin_interval = int(100/n_bin)\n",
    "\n",
    "    for i in range(n_bin):\n",
    "        lower_bound = i * bin_interval\n",
    "        upper_bound = lower_bound + bin_interval\n",
    "        curr_df = df[df[\"cr_word_acc\"].apply(\n",
    "            lambda x: x >= lower_bound and x < upper_bound)]\n",
    "        stat = stat.append(\n",
    "            {\n",
    "                \"bin_interval\": f\"{int(lower_bound)}-{int(upper_bound)}\",\n",
    "                \"pearson_corr\": scipy.stats.pearsonr(curr_df[\"ls_word_acc\"], curr_df[\"cr_word_acc\"])[0]}, ignore_index=True)\n",
    "\n",
    "stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat.to_csv(\"output/pearson_correlation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 0\n",
    "upper_bound = 10\n",
    "\n",
    "curr_df = df[df[\"ls_word_acc\"].apply(\n",
    "    lambda x: x >= lower_bound and x < upper_bound)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ls_word_acc</th>\n",
       "      <th>cr_word_acc</th>\n",
       "      <th>ls_word_count</th>\n",
       "      <th>cr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lelechka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fauchelevent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dickie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leslie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serafima</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>altar</td>\n",
       "      <td>8.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>o</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>albert</td>\n",
       "      <td>9.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>phoebe</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>pierre</td>\n",
       "      <td>9.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5011 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  ls_word_acc  cr_word_acc ls_word_count cr_word_count\n",
       "0         lelechka          0.0          0.0            35            35\n",
       "1     fauchelevent          0.0          0.0            24            24\n",
       "2           dickie          0.0          4.0            23            23\n",
       "3           leslie          0.0         91.0            23            23\n",
       "4         serafima          0.0          0.0            23            23\n",
       "...            ...          ...          ...           ...           ...\n",
       "5006         altar          8.0         83.0            12            12\n",
       "5007             o          9.0         33.0            85            85\n",
       "5008        albert          9.0         82.0            11            11\n",
       "5009        phoebe          9.0          9.0            11            11\n",
       "5010        pierre          9.0         64.0            11            11\n",
       "\n",
       "[5011 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.007846578110607077, 0.5786779764354835)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(curr_df[\"ls_word_acc\"], curr_df[\"cr_word_acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

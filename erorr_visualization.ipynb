{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Visualization\n",
    "\n",
    "- Quantifying Errors\n",
    "\t- From Determined Test Cases,  \n",
    "For each word, measure the number of *correctly* and *incorrectly* interpreted\n",
    "- Classifying Errors\n",
    "\t- Miss interpretation of the words  \n",
    "For each word, log the transcription, what is the the misinterpretation of the error  \n",
    "→ calculate the occurence  \n",
    "→ most common error for a given word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "import numpy\n",
    "import glob\n",
    "import jiwer\n",
    "import collections\n",
    "import helper\n",
    "\n",
    "# TODO: remove warning, put text preprocessing as helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reference = []\n",
    "        self.transcription = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        return helper.preprocess_text(text)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_reference(self):\n",
    "        return self.reference\n",
    "\n",
    "    def get_transcription(self):\n",
    "        return self.transcription\n",
    "\n",
    "    def add_reference(self, reference):\n",
    "        self.reference.append(self.preprocess_text(reference))\n",
    "    \n",
    "    def add_transcription(self, transcription):\n",
    "        self.transcription.append(self.preprocess_text(transcription))\n",
    "\n",
    "    def add_reference_transcription(self, reference, transcription):\n",
    "        self.add_reference(reference)\n",
    "        self.add_transcription(transcription)\n",
    "    \n",
    "    def length(self):\n",
    "        assert len(self.reference) == len(self.transcription)\n",
    "        return len(self.reference)\n",
    "\n",
    "    def print_reference_transcription(self, i):\n",
    "        if i >= 0 and i < len(self.reference):\n",
    "            print(\"Reference:   \\t: \", self.reference[i])\n",
    "            print(\"Transcription: \\t: \", self.transcription[i])\n",
    "\n",
    "    \n",
    "    def print_head(self):\n",
    "        self.print_reference_transcription(i=0)\n",
    "\n",
    "    def print_tail(self):\n",
    "        self.print_reference_transcription(i=self.length()-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_librispeech_data():\n",
    "    data = Data(\"librispeech\")\n",
    "    root_dir = \"LibriSpeech/test-clean/\"\n",
    "    model_dir = \"deepspeech\"\n",
    "\n",
    "    for filename in glob.iglob(root_dir + '**/*.trans.txt', recursive=True):\n",
    "        \n",
    "        file = open(filename)\n",
    "\n",
    "        for line in file.readlines():\n",
    "            idx = line.split()[0]\n",
    "            reference_text = \" \".join(line.split()[1:])\n",
    "\n",
    "            fid = \"/\".join(idx.split(\"-\")[:-1]) # idx to file id\n",
    "\n",
    "            fname = os.path.join(root_dir, fid, idx)\n",
    "            transcription_path = fname + \".\" + model_dir + \".transcription.txt\"\n",
    "            if os.path.exists(transcription_path):\n",
    "                transcription = helper.read_transcription(transcription_path)\n",
    "                data.add_reference_transcription(reference_text, transcription)\n",
    "            else:\n",
    "                raise ValueError(\"missing transcription: \" + transcription_path)\n",
    "\n",
    "        file.close()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  eleven oclock had struck it was a fine clear night there were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in the valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  then the leader parted from the line\n",
      "Transcription: \t:  then the leader parted from the line\n"
     ]
    }
   ],
   "source": [
    "data = read_librispeech_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_fpath: str):\n",
    "    file = open(corpus_fpath)\n",
    "    corpus = file.readlines()\n",
    "    texts = []\n",
    "    for text in corpus:\n",
    "        texts.append(text[:-1])\n",
    "\n",
    "    return texts\n",
    "\n",
    "def read_crossasr_data():\n",
    "    data = Data(\"crossasr\")\n",
    "\n",
    "    transcription_dir = \"CrossASR/europarl-seed2021/data/transcription\"\n",
    "    tts_name = \"rv\"\n",
    "    asr_name = \"deepspeech\"\n",
    "    transcription_dir = os.path.join(transcription_dir, tts_name)\n",
    "    transcription_dir = os.path.join(transcription_dir, asr_name)\n",
    "\n",
    "    references = read_corpus(\"CrossASR/europarl-seed2021/corpus/europarl-20000.txt\")\n",
    "    \n",
    "    for i in range(len(references)):\n",
    "        transcription_path = os.path.join(transcription_dir, f\"{i+1}.txt\")\n",
    "        transcription = helper.read_transcription(transcription_path)\n",
    "\n",
    "        data.add_reference_transcription(references[i], transcription)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  in the european year for intercultural dialogue we should also recognize the importance of cultural industries in creating awareness and understanding of other cultures and therefore their importance for social cohesion\n",
      "Transcription: \t:  in the european year for inter cultural dialogue we should also recognize the importance of cultural industries in creating awareness and understanding of other cultures and therefore their importance for social cohesion\n",
      "\n",
      "Reference:   \t:  it must be organised by a single body responsible for ensuring that it is applied comprehensively consistently and effectively\n",
      "Transcription: \t:  it must be organized by a single body responsible for ensuring that it is applied comprehensively consistently and effectively\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_evaluation.asr_evaluation import asr_evaluation\n",
    "\n",
    "class Analyzer(object):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## used for caching\n",
    "        self.infos = {}\n",
    "        self.word_count = {}\n",
    "        self.word_accuracy = {}\n",
    "        self.word_accuracy_with_count = {}\n",
    "        self.common_errors = {}\n",
    "\n",
    "        \n",
    "    def analyze(self, data: Data):\n",
    "\n",
    "        ## get from cache if it is already computed before\n",
    "        if data.get_name() in self.infos :\n",
    "            return self.infos[data.get_name()], self.word_count[data.get_name()]\n",
    "        \n",
    "        infos = []\n",
    "        word_count = collections.Counter()\n",
    "        \n",
    "        for reference, transcription, in zip(data.get_reference(), data.get_transcription()) :\n",
    "            \n",
    "            ## create statistics for word counter\n",
    "            word_count += collections.Counter(reference.split())\n",
    "            \n",
    "            ## create statistics for errors\n",
    "            wer = jiwer.wer(reference, transcription)\n",
    "            if wer != 0:\n",
    "                evaluation = asr_evaluation.ASREvaluation()\n",
    "                evaluation.detect_word_error(reference, transcription)\n",
    "                confusion = evaluation.get_confusions()\n",
    "                infos.append(\n",
    "                    {\"confusion\": confusion, \"reference\": reference, \"transcription\": transcription})\n",
    "        \n",
    "        ## update the cache\n",
    "        self.infos[data.get_name()] = infos\n",
    "        self.word_count[data.get_name()] = word_count\n",
    "        \n",
    "        \n",
    "        return infos, word_count\n",
    "\n",
    "    def get_word_accuracy(self, data: Data):\n",
    "        \"\"\"Calculate word accuracy, which is the number of error (deletion or subsitution) divided by the number of word count\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## use caching if it is already computed before\n",
    "        if data.get_name() in self.word_accuracy:\n",
    "            return self.word_accuracy[data.get_name()]\n",
    "\n",
    "        \n",
    "        infos, word_count = self.analyze(data)\n",
    "        error_count = collections.Counter()\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "\n",
    "            ## get error from word substitution\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                \n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "                \n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "            ## get error from word deletion\n",
    "            if len(confusion[\"deletion\"]) > 0:\n",
    "\n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"deletion\"])):\n",
    "                    word_reference = confusion[\"deletion\"][i][\"word\"]\n",
    "                    count = confusion[\"deletion\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "\n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "        \n",
    "        word_accuracy = {}\n",
    "        word_accuracy_with_count = {}\n",
    "        for word in word_count :\n",
    "            \n",
    "            ## if the word in the reference_text does not appear in the error word\n",
    "            ## then all the occurence of the word is correctly predicted \n",
    "            if word not in error_count :\n",
    "                word_accuracy[word] = 100.0 \n",
    "                word_accuracy_with_count[word] = [100.0 , word_count[word]]\n",
    "            else :\n",
    "                curr_word_accuracy = 100.0 - (100 * round(error_count[word]/word_count[word], 2))\n",
    "                assert curr_word_accuracy <= 100 and curr_word_accuracy >= 0\n",
    "                word_accuracy[word] = curr_word_accuracy\n",
    "                word_accuracy_with_count[word] = [curr_word_accuracy, word_count[word]]\n",
    "\n",
    "        ## sort the word accuracy based on the word_accuracy\n",
    "        word_accuracy = dict(sorted(word_accuracy.items(),\n",
    "                                    key=lambda item: (item[1], item[0]), reverse=True))\n",
    "\n",
    "        ## sort the word accuracy with count based on the word_accuracy\n",
    "        word_accuracy_with_count = dict(sorted(word_accuracy_with_count.items(),\n",
    "                                               key=lambda item: (item[1][0], -item[1][1], item[0]), reverse=False))\n",
    "\n",
    "        \n",
    "        ## update cache\n",
    "        self.word_accuracy[data.get_name()] = word_accuracy\n",
    "        self.word_accuracy_with_count[data.get_name()] = word_accuracy_with_count\n",
    "        \n",
    "        return word_accuracy\n",
    "\n",
    "    def print_word_accuracy_with_minimum_count(self, data: Data, limit=0, minimium_count=10, ascending=True):\n",
    "        \n",
    "        if not data.get_name() in self.word_accuracy_with_count :\n",
    "            self.get_word_accuracy(data)\n",
    "        \n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[data.get_name()]\n",
    "\n",
    "        # ## sort the word accuracy based on the value\n",
    "        # sorting_key = 0  ## 0 for sorting using word_accuracy, 1 for sorting using word_count\n",
    "        # word_accuracy_with_count = dict(sorted(word_accuracy_with_count.items(),\n",
    "        #                             key=lambda item: item[1][sorting_key], reverse=True))\n",
    "\n",
    "    \n",
    "        keys = list(word_accuracy_with_count.keys())\n",
    "        values = list(word_accuracy_with_count.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'} \\tCount\")\n",
    "        \n",
    "        j = 0\n",
    "        for i in reversed(range(len(keys))) if ascending else range(len(keys)):\n",
    "            if j < limit :\n",
    "                if values[i][1] >= minimium_count :\n",
    "                    print(f\"{keys[i]:15s} {values[i][0]:} \\t\\t{values[i][1]:}\")\n",
    "                    j += 1\n",
    "\n",
    "\n",
    "    def save_word_accuracy(self, dataset_name, fpath):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy.keys())\n",
    "            values = list(word_accuracy.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'}\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(f\"{keys[i]:15s} \\t{values[i]}\\n\")\n",
    "\n",
    "    def save_word_accuracy_with_count(self, dataset_name, fpath):\n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy_with_count.keys())\n",
    "            values = list(word_accuracy_with_count.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'} \\tCount\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(\n",
    "                    f\"{keys[i]:15s} \\t{values[i][0]} \\t{values[i][1]}\\n\")\n",
    "\n",
    "    def print_lowest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(len(keys)-limit, len(keys)):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "    def print_highest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(limit):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "\n",
    "    def get_most_common_errors(self, data: Data):\n",
    "        \n",
    "        \n",
    "        ## geta result from caching if it is already computed before\n",
    "        if data.get_name() in self.common_errors:\n",
    "            return self.common_errors[data.get_name()]\n",
    "\n",
    "\n",
    "        infos, _ = self.analyze(data)\n",
    "\n",
    "        ## TODO: use Counter library\n",
    "        common_errors = {}\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    word_substitution = confusion[\"substitution\"][i][\"word_substitution\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "                    if word_reference in common_errors:\n",
    "                        substitutions = common_errors[word_reference]\n",
    "                        if word_substitution in substitutions:\n",
    "                            common_errors[word_reference][word_substitution] = count + \\\n",
    "                                common_errors[word_reference][word_substitution]\n",
    "                        else:\n",
    "                            common_errors[word_reference][word_substitution] = count\n",
    "                    else:\n",
    "                        common_errors[word_reference] = {\n",
    "                            word_substitution: count}\n",
    "\n",
    "        ## sort things inside the common error\n",
    "        for key in common_errors.keys():\n",
    "            common_errors[key] = dict(sorted(common_errors[key].items(),\n",
    "                                            key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        ## sort words based on the highest occurence\n",
    "        common_errors = dict(sorted(common_errors.items(),\n",
    "                                        key=lambda item: list(item[1].values())[0], reverse=True))\n",
    "        \n",
    "        self.common_errors[data.get_name()] = common_errors\n",
    "        \n",
    "        return common_errors\n",
    "\n",
    "    def print_common_error(self, common_errors, limit=2):\n",
    "        count = 0\n",
    "        print_limit = 16\n",
    "        for word, common in common_errors.items():\n",
    "            if count < print_limit :\n",
    "                print(\"Word: \", word)\n",
    "                # print(\"Substituion: \")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(min(limit, len(keys))):\n",
    "                    print(f\"\\t{keys[i]:10s} count: {values[i]}\")\n",
    "            count += 1\n",
    "\n",
    "    def save_common_errors(self, common_errors, fpath):\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            for word, common in common_errors.items():\n",
    "                file.write(f\"Word: {word}\\n\")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(len(keys)):\n",
    "                    file.write(f\"\\t{keys[i]:10s} count: {values[i]}\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "advised         0.0\n",
      "adona           0.0\n",
      "admittance      0.0\n",
      "adherents       0.0\n",
      "acknowledgement 0.0\n",
      "accruing        0.0\n",
      "abstractions    0.0\n",
      "abolitionists   0.0\n",
      "abduction       0.0\n",
      "abbe            0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zoology         100.0\n",
      "zion            100.0\n",
      "zeal            100.0\n",
      "youth           100.0\n",
      "yourselves      100.0\n",
      "yourself        100.0\n",
      "yours           100.0\n",
      "younger         100.0\n",
      "yorkshire       100.0\n",
      "york            100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer()\n",
    "\n",
    "data = read_librispeech_data()\n",
    "\n",
    "word_accuracy = analyzer.get_word_accuracy(data)\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "wrong           100.0 \t\t10\n",
      "simply          100.0 \t\t10\n",
      "show            100.0 \t\t10\n",
      "shook           100.0 \t\t10\n",
      "second          100.0 \t\t10\n",
      "remember        100.0 \t\t10\n",
      "really          100.0 \t\t10\n",
      "ready           100.0 \t\t10\n",
      "purpose         100.0 \t\t10\n",
      "probably        100.0 \t\t10\n",
      "pretty          100.0 \t\t10\n",
      "ought           100.0 \t\t10\n",
      "noble           100.0 \t\t10\n",
      "nearly          100.0 \t\t10\n",
      "natural         100.0 \t\t10\n",
      "mistress        100.0 \t\t10\n",
      "met             100.0 \t\t10\n",
      "keep            100.0 \t\t10\n",
      "hung            100.0 \t\t10\n",
      "ground          100.0 \t\t10\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  in\n",
      "\tand        count: 44\n",
      "\tan         count: 3\n",
      "Word:  a\n",
      "\tthe        count: 34\n",
      "\tof         count: 4\n",
      "Word:  and\n",
      "\tin         count: 28\n",
      "\ta          count: 3\n",
      "Word:  this\n",
      "\tthe        count: 21\n",
      "\tspilling   count: 1\n",
      "Word:  an\n",
      "\tand        count: 13\n",
      "\ton         count: 1\n",
      "Word:  too\n",
      "\tto         count: 10\n",
      "\ttwo        count: 3\n",
      "Word:  two\n",
      "\tto         count: 8\n",
      "\tlotto      count: 1\n",
      "Word:  is\n",
      "\tas         count: 7\n",
      "\this        count: 4\n",
      "Word:  the\n",
      "\ta          count: 7\n",
      "\tto         count: 2\n",
      "Word:  uncas\n",
      "\tonce       count: 6\n",
      "\tone        count: 1\n",
      "Word:  thee\n",
      "\tthe        count: 6\n",
      "\the         count: 4\n",
      "Word:  of\n",
      "\ta          count: 6\n",
      "\tat         count: 2\n",
      "Word:  anyone\n",
      "\tone        count: 6\n",
      "Word:  men\n",
      "\tman        count: 6\n",
      "\tthen       count: 1\n",
      "Word:  boolooroo\n",
      "\tbolero     count: 6\n",
      "\tbooleroo   count: 4\n",
      "Word:  has\n",
      "\thad        count: 5\n",
      "\tas         count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/librispeech/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "abidjan         0.0\n",
      "abhorrent       0.0\n",
      "abe             0.0\n",
      "abbey           0.0\n",
      "abattoirs       0.0\n",
      "abattoir        0.0\n",
      "abacha          0.0\n",
      "ababa           0.0\n",
      "aarhus          0.0\n",
      "aachen          0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zuma            100.0\n",
      "zealand         100.0\n",
      "zeal            100.0\n",
      "zaragoza        100.0\n",
      "zapatero        100.0\n",
      "youth           100.0\n",
      "yourselves      100.0\n",
      "yourself        100.0\n",
      "yours           100.0\n",
      "youngsters      100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "\n",
    "analyzer = Analyzer()\n",
    "word_accuracy = analyzer.get_word_accuracy(data)\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  and\n",
      "\tin         count: 1842\n",
      "\tan         count: 154\n",
      "Word:  today\n",
      "\tday        count: 336\n",
      "\tto         count: 8\n",
      "Word:  is\n",
      "\tas         count: 281\n",
      "\this        count: 42\n",
      "Word:  a\n",
      "\tthe        count: 281\n",
      "\tof         count: 80\n",
      "Word:  has\n",
      "\thad        count: 265\n",
      "\thave       count: 8\n",
      "Word:  favour\n",
      "\tfavor      count: 207\n",
      "\tto         count: 1\n",
      "Word:  rapporteur\n",
      "\treporter   count: 183\n",
      "\trapier     count: 16\n",
      "Word:  cooperation\n",
      "\toperation  count: 163\n",
      "\tation      count: 1\n",
      "Word:  member\n",
      "\tmembers    count: 149\n",
      "\tember      count: 2\n",
      "Word:  programmes\n",
      "\tprograms   count: 145\n",
      "\tprogram    count: 7\n",
      "Word:  s\n",
      "\tas         count: 144\n",
      "\tus         count: 67\n",
      "Word:  as\n",
      "\tis         count: 140\n",
      "\this        count: 13\n",
      "Word:  thank\n",
      "\tthink      count: 138\n",
      "\tevery      count: 2\n",
      "Word:  ensure\n",
      "\tinsure     count: 128\n",
      "\tinsured    count: 8\n",
      "Word:  mr\n",
      "\tr          count: 123\n",
      "\tm          count: 37\n",
      "Word:  eu\n",
      "\te          count: 120\n",
      "\tu          count: 74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/crossasr/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "voters          100.0 \t\t10\n",
      "volume          100.0 \t\t10\n",
      "vocational      100.0 \t\t10\n",
      "virtually       100.0 \t\t10\n",
      "vienna          100.0 \t\t10\n",
      "variety         100.0 \t\t10\n",
      "usual           100.0 \t\t10\n",
      "uses            100.0 \t\t10\n",
      "underlying      100.0 \t\t10\n",
      "transmission    100.0 \t\t10\n",
      "thoroughly      100.0 \t\t10\n",
      "tankers         100.0 \t\t10\n",
      "systematically  100.0 \t\t10\n",
      "swiftly         100.0 \t\t10\n",
      "supreme         100.0 \t\t10\n",
      "subsequent      100.0 \t\t10\n",
      "student         100.0 \t\t10\n",
      "speaks          100.0 \t\t10\n",
      "sold            100.0 \t\t10\n",
      "smoking         100.0 \t\t10\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(\n",
    "    data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight \n",
    "\n",
    "It seems hard to compare the words intersection one-by-one. \n",
    "We will try qeurying the statistic for each word that has much error in CrossASR\n",
    "Then get corresponding statistic from Librispeech data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

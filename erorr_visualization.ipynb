{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Visualization\n",
    "\n",
    "- Quantifying Errors\n",
    "\t- From Determined Test Cases,  \n",
    "For each word, measure the number of *correctly* and *incorrectly* interpreted\n",
    "- Classifying Errors\n",
    "\t- Miss interpretation of the words  \n",
    "For each word, log the transcription, what is the the misinterpretation of the error  \n",
    "â†’ calculate the occurence  \n",
    "â†’ most common error for a given word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "import numpy\n",
    "import glob\n",
    "import jiwer\n",
    "import collections\n",
    "import helper\n",
    "\n",
    "# TODO: remove warning, put text preprocessing as helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reference = []\n",
    "        self.transcription = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        return helper.preprocess_text(text)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_reference(self):\n",
    "        return self.reference\n",
    "\n",
    "    def get_transcription(self):\n",
    "        return self.transcription\n",
    "\n",
    "    def add_reference(self, reference):\n",
    "        self.reference.append(self.preprocess_text(reference))\n",
    "    \n",
    "    def add_transcription(self, transcription):\n",
    "        self.transcription.append(self.preprocess_text(transcription))\n",
    "\n",
    "    def add_reference_transcription(self, reference, transcription):\n",
    "        self.add_reference(reference)\n",
    "        self.add_transcription(transcription)\n",
    "    \n",
    "    def length(self):\n",
    "        assert len(self.reference) == len(self.transcription)\n",
    "        return len(self.reference)\n",
    "\n",
    "    def print_reference_transcription(self, i):\n",
    "        if i >= 0 and i < len(self.reference):\n",
    "            print(\"Reference:   \\t: \", self.reference[i])\n",
    "            print(\"Transcription: \\t: \", self.transcription[i])\n",
    "\n",
    "    \n",
    "    def print_head(self):\n",
    "        self.print_reference_transcription(i=0)\n",
    "\n",
    "    def print_tail(self):\n",
    "        self.print_reference_transcription(i=self.length()-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_librispeech_data():\n",
    "    data = Data(\"librispeech\")\n",
    "    root_dir = \"LibriSpeech/test-clean/\"\n",
    "    model_dir = \"deepspeech\"\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for filename in glob.iglob(root_dir + '**/*.trans.txt', recursive=True):\n",
    "        \n",
    "        file = open(filename)\n",
    "\n",
    "        for line in file.readlines():\n",
    "            idx = line.split()[0]\n",
    "            reference_text = \" \".join(line.split()[1:])\n",
    "\n",
    "            fid = \"/\".join(idx.split(\"-\")[:-1]) # idx to file id\n",
    "\n",
    "            fname = os.path.join(root_dir, fid, idx)\n",
    "            transcription_path = fname + \".\" + model_dir + \".transcription.txt\"\n",
    "            if os.path.exists(transcription_path):\n",
    "                transcription = helper.read_transcription(transcription_path)\n",
    "                data.add_reference_transcription(reference_text, transcription)\n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(\"missing transcription: \" + transcription_path)\n",
    "\n",
    "        file.close()\n",
    "    \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  eleven oclock had struck it was a fine clear night there were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in the valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  then the leader parted from the line\n",
      "Transcription: \t:  then the leader parted from the line\n"
     ]
    }
   ],
   "source": [
    "data = read_librispeech_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_fpath: str):\n",
    "    file = open(corpus_fpath)\n",
    "    corpus = file.readlines()\n",
    "    texts = []\n",
    "    for text in corpus:\n",
    "        texts.append(text[:-1])\n",
    "\n",
    "    return texts\n",
    "\n",
    "def read_crossasr_data():\n",
    "    data = Data(\"crossasr\")\n",
    "\n",
    "    mode = \"europarl\"\n",
    "    mode = \"librispeech\"\n",
    "\n",
    "    if mode == \"europarl\" :\n",
    "        corpus_fpath = \"CrossASR/europarl-seed2021/corpus/europarl-20000.txt\"\n",
    "        transcription_dir = \"CrossASR/europarl-seed2021/data/transcription\"\n",
    "        tts_name = \"rv\"\n",
    "    elif mode == \"librispeech\" :\n",
    "        corpus_fpath = \"CrossASR/librispeech-crossasr/corpus/librispeech-test-clean-corpus.txt\"\n",
    "        transcription_dir = \"CrossASR/librispeech-crossasr/data/transcription\"\n",
    "        tts_name = \"google\"\n",
    "\n",
    "    \n",
    "    asr_name = \"deepspeech\"\n",
    "    transcription_dir = os.path.join(transcription_dir, tts_name)\n",
    "    transcription_dir = os.path.join(transcription_dir, asr_name)\n",
    "\n",
    "    references = read_corpus(corpus_fpath)\n",
    "    \n",
    "    for i in range(len(references)):\n",
    "        if mode == \"europarl\" :\n",
    "            transcription_path = os.path.join(transcription_dir, f\"{i+1}.txt\")\n",
    "        elif mode == \"librispeech\" :\n",
    "            transcription_path = os.path.join(transcription_dir, f\"{i}.txt\")\n",
    "        transcription = helper.read_transcription(transcription_path)\n",
    "\n",
    "        data.add_reference_transcription(references[i], transcription)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  oclock had struck it was a fine clear night they were the only persons on the road and day sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  then the leader parted from the line\n",
      "Transcription: \t:  then the leader parted from the line\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_evaluation.asr_evaluation import asr_evaluation\n",
    "\n",
    "class Analyzer(object):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## used for caching\n",
    "        self.infos = {}\n",
    "        self.word_count = {}\n",
    "        self.word_accuracy = {}\n",
    "        self.word_accuracy_with_count = {}\n",
    "        self.common_errors = {}\n",
    "\n",
    "        \n",
    "    def analyze(self, data: Data):\n",
    "\n",
    "        ## get from cache if it is already computed before\n",
    "        if data.get_name() in self.infos :\n",
    "            return self.infos[data.get_name()], self.word_count[data.get_name()]\n",
    "        \n",
    "        infos = []\n",
    "        word_count = collections.Counter()\n",
    "        \n",
    "        for reference, transcription, in zip(data.get_reference(), data.get_transcription()) :\n",
    "            \n",
    "            ## create statistics for word counter\n",
    "            word_count += collections.Counter(reference.split())\n",
    "            \n",
    "            ## create statistics for errors\n",
    "            wer = jiwer.wer(reference, transcription)\n",
    "            if wer != 0:\n",
    "                evaluation = asr_evaluation.ASREvaluation()\n",
    "                evaluation.detect_word_error(reference, transcription)\n",
    "                confusion = evaluation.get_confusions()\n",
    "                infos.append(\n",
    "                    {\"confusion\": confusion, \"reference\": reference, \"transcription\": transcription})\n",
    "        \n",
    "        ## update the cache\n",
    "        self.infos[data.get_name()] = infos\n",
    "        self.word_count[data.get_name()] = word_count\n",
    "        \n",
    "        \n",
    "        return infos, word_count\n",
    "\n",
    "    def calculate_word_accuracy(self, data: Data):\n",
    "        \"\"\"Calculate word accuracy, which is the number of error (deletion or subsitution) divided by the number of word count\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## use caching if it is already computed before\n",
    "        if data.get_name() in self.word_accuracy:\n",
    "            return self.word_accuracy[data.get_name()]\n",
    "\n",
    "        \n",
    "        infos, word_count = self.analyze(data)\n",
    "        error_count = collections.Counter()\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "\n",
    "            ## get error from word substitution\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                \n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "                \n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "            ## get error from word deletion\n",
    "            if len(confusion[\"deletion\"]) > 0:\n",
    "\n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"deletion\"])):\n",
    "                    word_reference = confusion[\"deletion\"][i][\"word\"]\n",
    "                    count = confusion[\"deletion\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "\n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "        \n",
    "        word_accuracy = {}\n",
    "        word_accuracy_with_count = {}\n",
    "        for word in word_count :\n",
    "            \n",
    "            ## if the word in the reference_text does not appear in the error word\n",
    "            ## then all the occurence of the word is correctly predicted \n",
    "            if word not in error_count :\n",
    "                word_accuracy[word] = 100.0 \n",
    "                word_accuracy_with_count[word] = [100.0 , word_count[word]]\n",
    "            else :\n",
    "                curr_word_accuracy = 100.0 - (100 * round(error_count[word]/word_count[word], 2))\n",
    "                assert curr_word_accuracy <= 100 and curr_word_accuracy >= 0\n",
    "                word_accuracy[word] = curr_word_accuracy\n",
    "                word_accuracy_with_count[word] = [curr_word_accuracy, word_count[word]]\n",
    "\n",
    "        ## sort the word accuracy based on the word_accuracy\n",
    "        word_accuracy = dict(sorted(word_accuracy.items(),\n",
    "                                    key=lambda item: (item[1], item[0]), reverse=True))\n",
    "\n",
    "        ## sort the word accuracy with count based on the word_accuracy\n",
    "        word_accuracy_with_count = dict(sorted(word_accuracy_with_count.items(),\n",
    "                                               key=lambda item: (item[1][0], -item[1][1], item[0]), reverse=False))\n",
    "\n",
    "        \n",
    "        ## update cache\n",
    "        self.word_accuracy[data.get_name()] = word_accuracy\n",
    "        self.word_accuracy_with_count[data.get_name()] = word_accuracy_with_count\n",
    "        \n",
    "        return word_accuracy\n",
    "\n",
    "    def get_word_accuracy(self, dataset_name:str):\n",
    "        if dataset_name in self.word_accuracy :\n",
    "            return self.word_accuracy[dataset_name]\n",
    "        return None\n",
    "\n",
    "    def get_word_accuracy_with_count(self, dataset_name: str):\n",
    "        if dataset_name in self.word_accuracy_with_count:\n",
    "            return self.word_accuracy_with_count[dataset_name]\n",
    "        return None\n",
    "\n",
    "\n",
    "    def print_word_accuracy_with_minimum_count(self, data: Data, limit=0, minimium_count=10, ascending=True):\n",
    "        \n",
    "        if not data.get_name() in self.word_accuracy_with_count :\n",
    "            self.calculate_word_accuracy(data)\n",
    "        \n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[data.get_name()]\n",
    "    \n",
    "        keys = list(word_accuracy_with_count.keys())\n",
    "        values = list(word_accuracy_with_count.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'} \\tCount\")\n",
    "        \n",
    "        j = 0\n",
    "        for i in reversed(range(len(keys))) if ascending else range(len(keys)):\n",
    "            if j < limit :\n",
    "                if values[i][1] >= minimium_count :\n",
    "                    print(f\"{keys[i]:15s} {values[i][0]:} \\t\\t{values[i][1]:}\")\n",
    "                    j += 1\n",
    "\n",
    "\n",
    "    def save_word_accuracy(self, dataset_name, fpath):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy.keys())\n",
    "            values = list(word_accuracy.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'}\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(f\"{keys[i]:15s} \\t{values[i]}\\n\")\n",
    "\n",
    "    def save_word_accuracy_with_count(self, dataset_name, fpath):\n",
    "        word_accuracy_with_count = self.word_accuracy_with_count[dataset_name]\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy_with_count.keys())\n",
    "            values = list(word_accuracy_with_count.values())\n",
    "            file.write(f\"{'Word':15s} \\t{'Accuracy'} \\tCount\\n\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(\n",
    "                    f\"{keys[i]:15s} \\t{values[i][0]} \\t{values[i][1]}\\n\")\n",
    "\n",
    "    def print_lowest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(len(keys)-limit, len(keys)):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "    def print_highest_word_accuracy(self, dataset_name, limit=10):\n",
    "        word_accuracy = self.word_accuracy[dataset_name]\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(limit):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "\n",
    "    def get_most_common_errors(self, data: Data):\n",
    "        \n",
    "        \n",
    "        ## geta result from caching if it is already computed before\n",
    "        if data.get_name() in self.common_errors:\n",
    "            return self.common_errors[data.get_name()]\n",
    "\n",
    "\n",
    "        infos, _ = self.analyze(data)\n",
    "\n",
    "        ## TODO: use Counter library\n",
    "        common_errors = {}\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    word_substitution = confusion[\"substitution\"][i][\"word_substitution\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    if word_reference == \"and\" and word_substitution == \"terrified\":\n",
    "                        print(info[\"reference\"])\n",
    "                        print(info[\"transcription\"])\n",
    "\n",
    "\n",
    "                    if word_reference in common_errors:\n",
    "                        substitutions = common_errors[word_reference]\n",
    "                        if word_substitution in substitutions:\n",
    "                            common_errors[word_reference][word_substitution] = count + \\\n",
    "                                common_errors[word_reference][word_substitution]\n",
    "                        else:\n",
    "                            common_errors[word_reference][word_substitution] = count\n",
    "                    else:\n",
    "                        common_errors[word_reference] = {\n",
    "                            word_substitution: count}\n",
    "\n",
    "        ## sort things inside the common error\n",
    "        for key in common_errors.keys():\n",
    "            common_errors[key] = dict(sorted(common_errors[key].items(),\n",
    "                                            key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        ## sort words based on the highest occurence\n",
    "        common_errors = dict(sorted(common_errors.items(),\n",
    "                                        key=lambda item: list(item[1].values())[0], reverse=True))\n",
    "        \n",
    "        self.common_errors[data.get_name()] = common_errors\n",
    "        \n",
    "        return common_errors\n",
    "\n",
    "    def print_common_error(self, common_errors, limit=2):\n",
    "        count = 0\n",
    "        print_limit = 16\n",
    "        for word, common in common_errors.items():\n",
    "            if count < print_limit :\n",
    "                print(\"Word: \", word)\n",
    "                # print(\"Substituion: \")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(min(limit, len(keys))):\n",
    "                    print(f\"\\t{keys[i]:10s} count: {values[i]}\")\n",
    "            count += 1\n",
    "\n",
    "    def save_common_errors(self, common_errors, fpath):\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            for word, common in common_errors.items():\n",
    "                file.write(f\"Word: {word}\\n\")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(len(keys)):\n",
    "                    file.write(f\"\\t{keys[i]:10s} count: {values[i]}\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "advised         0.0\n",
      "adona           0.0\n",
      "admittance      0.0\n",
      "adherents       0.0\n",
      "acknowledgement 0.0\n",
      "accruing        0.0\n",
      "abstractions    0.0\n",
      "abolitionists   0.0\n",
      "abduction       0.0\n",
      "abbe            0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zoology         100.0\n",
      "zion            100.0\n",
      "zeal            100.0\n",
      "youth           100.0\n",
      "yourselves      100.0\n",
      "yourself        100.0\n",
      "yours           100.0\n",
      "younger         100.0\n",
      "yorkshire       100.0\n",
      "york            100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer()\n",
    "\n",
    "data = read_librispeech_data()\n",
    "\n",
    "analyzer.calculate_word_accuracy(data)\n",
    "word_accuracy = analyzer.get_word_accuracy(data.get_name())\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/librispeech/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "wrong           100.0 \t\t10\n",
      "simply          100.0 \t\t10\n",
      "show            100.0 \t\t10\n",
      "shook           100.0 \t\t10\n",
      "second          100.0 \t\t10\n",
      "remember        100.0 \t\t10\n",
      "really          100.0 \t\t10\n",
      "ready           100.0 \t\t10\n",
      "purpose         100.0 \t\t10\n",
      "probably        100.0 \t\t10\n",
      "pretty          100.0 \t\t10\n",
      "ought           100.0 \t\t10\n",
      "noble           100.0 \t\t10\n",
      "nearly          100.0 \t\t10\n",
      "natural         100.0 \t\t10\n",
      "mistress        100.0 \t\t10\n",
      "met             100.0 \t\t10\n",
      "keep            100.0 \t\t10\n",
      "hung            100.0 \t\t10\n",
      "ground          100.0 \t\t10\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  in\n",
      "\tand        count: 44\n",
      "\tan         count: 3\n",
      "Word:  a\n",
      "\tthe        count: 34\n",
      "\tof         count: 4\n",
      "Word:  and\n",
      "\tin         count: 28\n",
      "\ta          count: 3\n",
      "Word:  this\n",
      "\tthe        count: 21\n",
      "\tspilling   count: 1\n",
      "Word:  an\n",
      "\tand        count: 13\n",
      "\ton         count: 1\n",
      "Word:  too\n",
      "\tto         count: 10\n",
      "\ttwo        count: 3\n",
      "Word:  two\n",
      "\tto         count: 8\n",
      "\tlotto      count: 1\n",
      "Word:  is\n",
      "\tas         count: 7\n",
      "\this        count: 4\n",
      "Word:  the\n",
      "\ta          count: 7\n",
      "\tto         count: 2\n",
      "Word:  uncas\n",
      "\tonce       count: 6\n",
      "\tone        count: 1\n",
      "Word:  thee\n",
      "\tthe        count: 6\n",
      "\the         count: 4\n",
      "Word:  of\n",
      "\ta          count: 6\n",
      "\tat         count: 2\n",
      "Word:  anyone\n",
      "\tone        count: 6\n",
      "Word:  men\n",
      "\tman        count: 6\n",
      "\tthen       count: 1\n",
      "Word:  boolooroo\n",
      "\tbolero     count: 6\n",
      "\tbooleroo   count: 4\n",
      "Word:  has\n",
      "\thad        count: 5\n",
      "\tas         count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/librispeech/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "adona           0.0\n",
      "addressing      0.0\n",
      "acknowledgement 0.0\n",
      "achievements    0.0\n",
      "accounts        0.0\n",
      "accordingly     0.0\n",
      "accomplishment  0.0\n",
      "accepting       0.0\n",
      "abner           0.0\n",
      "abbe            0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "zoology         100.0\n",
      "zest            100.0\n",
      "zeal            100.0\n",
      "youth           100.0\n",
      "younger         100.0\n",
      "yorkshire       100.0\n",
      "york            100.0\n",
      "yonder          100.0\n",
      "yoke            100.0\n",
      "yielding        100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "\n",
    "# analyzer = Analyzer()\n",
    "analyzer.calculate_word_accuracy(data)\n",
    "word_accuracy = analyzer.get_word_accuracy(data.get_name())\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(data.get_name(), fpath)\n",
    "\n",
    "fpath = \"output/crossasr/word_accuracy_with_count.txt\"\n",
    "analyzer.save_word_accuracy_with_count(data.get_name(), fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(data.get_name())\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(data.get_name())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  as\n",
      "\tis         count: 50\n",
      "\this        count: 2\n",
      "Word:  and\n",
      "\tin         count: 44\n",
      "\tthe        count: 3\n",
      "Word:  the\n",
      "\tthat       count: 22\n",
      "\tthere      count: 9\n",
      "Word:  was\n",
      "\twith       count: 17\n",
      "\tas         count: 4\n",
      "Word:  who\n",
      "\the         count: 16\n",
      "\tyou        count: 5\n",
      "Word:  a\n",
      "\tthe        count: 14\n",
      "\tof         count: 8\n",
      "Word:  our\n",
      "\ta          count: 13\n",
      "\tus         count: 4\n",
      "Word:  too\n",
      "\tto         count: 13\n",
      "\ttwo        count: 5\n",
      "Word:  an\n",
      "\tand        count: 13\n",
      "\tin         count: 8\n",
      "Word:  im\n",
      "\tin         count: 12\n",
      "\thim        count: 3\n",
      "Word:  are\n",
      "\tand        count: 10\n",
      "\ta          count: 7\n",
      "Word:  them\n",
      "\thim        count: 10\n",
      "\tthen       count: 3\n",
      "Word:  their\n",
      "\tthe        count: 10\n",
      "\tthere      count: 2\n",
      "Word:  you\n",
      "\the         count: 9\n",
      "\tthe        count: 3\n",
      "Word:  at\n",
      "\tit         count: 9\n",
      "\tthat       count: 2\n",
      "Word:  were\n",
      "\twas        count: 8\n",
      "\twith       count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/crossasr/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Accuracy \tCount\n",
      "talking         100.0 \t\t10\n",
      "speaking        100.0 \t\t10\n",
      "simply          100.0 \t\t10\n",
      "second          100.0 \t\t10\n",
      "remember        100.0 \t\t10\n",
      "really          100.0 \t\t10\n",
      "ready           100.0 \t\t10\n",
      "purpose         100.0 \t\t10\n",
      "probably        100.0 \t\t10\n",
      "pretty          100.0 \t\t10\n",
      "ought           100.0 \t\t10\n",
      "noble           100.0 \t\t10\n",
      "nearly          100.0 \t\t10\n",
      "natural         100.0 \t\t10\n",
      "moved           100.0 \t\t10\n",
      "mistress        100.0 \t\t10\n",
      "met             100.0 \t\t10\n",
      "knife           100.0 \t\t10\n",
      "hung            100.0 \t\t10\n",
      "hope            100.0 \t\t10\n"
     ]
    }
   ],
   "source": [
    "analyzer.print_word_accuracy_with_minimum_count(\n",
    "    data, limit=20, minimium_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight \n",
    "\n",
    "It seems hard to compare the words intersection one-by-one. \n",
    "We will try qeurying the statistic for each word that has much error in CrossASR\n",
    "Then get corresponding statistic from Librispeech data\n",
    "\n",
    "### Combining Word Accuracy from the Two Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_data = read_crossasr_data()   ## crossasr dataq\n",
    "ls_data = read_librispeech_data()   ## librispeech data\n",
    "\n",
    "analyzer = Analyzer()\n",
    "\n",
    "analyzer.calculate_word_accuracy(cr_data)\n",
    "analyzer.calculate_word_accuracy(ls_data)\n",
    "\n",
    "\n",
    "cr_word_acc = analyzer.get_word_accuracy_with_count(cr_data.get_name())\n",
    "ls_word_acc = analyzer.get_word_accuracy_with_count(ls_data.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each error in librispeech word accuracy, inform the corresponding error on the crossasr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ls_word_acc</th>\n",
       "      <th>cr_word_acc</th>\n",
       "      <th>ls_word_count</th>\n",
       "      <th>cr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boolooroo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>timaeus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fitzooth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gamewell</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anyone</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8095</th>\n",
       "      <td>yell</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>younger</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8097</th>\n",
       "      <td>yourselves</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8098</th>\n",
       "      <td>zion</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8099</th>\n",
       "      <td>zoology</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  ls_word_acc  cr_word_acc ls_word_count cr_word_count\n",
       "0      boolooroo          0.0          0.0            12            12\n",
       "1        timaeus          0.0         11.0             9             9\n",
       "2       fitzooth          0.0          0.0             7             7\n",
       "3       gamewell          0.0          0.0             7             7\n",
       "4         anyone          0.0         17.0             6             6\n",
       "...          ...          ...          ...           ...           ...\n",
       "8095        yell        100.0        100.0             1             1\n",
       "8096     younger        100.0        100.0             1             1\n",
       "8097  yourselves        100.0          0.0             1             1\n",
       "8098        zion        100.0          0.0             1             1\n",
       "8099     zoology        100.0        100.0             1             1\n",
       "\n",
       "[8100 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"word\", \"ls_word_acc\",\n",
    "                  \"cr_word_acc\", \"ls_word_count\",  \"cr_word_count\"])\n",
    "for word in ls_word_acc :\n",
    "    if word in cr_word_acc :\n",
    "        df = df.append({\"word\": word,\n",
    "                        \"ls_word_acc\": ls_word_acc[word][0],\n",
    "                        \"cr_word_acc\": cr_word_acc[word][0],\n",
    "                        \"ls_word_count\": ls_word_acc[word][1],\n",
    "                        \"cr_word_count\": cr_word_acc[word][1]\n",
    "                        }, ignore_index=True)\n",
    "    else :\n",
    "        df = df.append({\"word\": word,\n",
    "                        \"ls_word_acc\": ls_word_acc[word][0], \n",
    "                        \"cr_word_acc\" : -1,\n",
    "                        \"ls_word_count\": ls_word_acc[word][1],\n",
    "                        \"cr_word_count\": -1\n",
    "                   }, ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/combined_word_accuracy.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

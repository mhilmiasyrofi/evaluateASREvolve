{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Visualization\n",
    "\n",
    "- Quantifying Errors\n",
    "\t- From Determined Test Cases,  \n",
    "For each word, measure the number of *correctly* and *incorrectly* interpreted\n",
    "- Classifying Errors\n",
    "\t- Miss interpretation of the words  \n",
    "For each word, log the transcription, what is the the misinterpretation of the error  \n",
    "â†’ calculate the occurence  \n",
    "â†’ most common error for a given word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/masyrofi/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "import numpy\n",
    "import glob\n",
    "import jiwer\n",
    "import collections\n",
    "import helper\n",
    "\n",
    "# TODO: remove warning, put text preprocessing as helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reference = []\n",
    "        self.transcription = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        return helper.preprocess_text(text)\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_reference(self):\n",
    "        return self.reference\n",
    "\n",
    "    def get_transcription(self):\n",
    "        return self.transcription\n",
    "\n",
    "    def add_reference(self, reference):\n",
    "        self.reference.append(self.preprocess_text(reference))\n",
    "    \n",
    "    def add_transcription(self, transcription):\n",
    "        self.transcription.append(self.preprocess_text(transcription))\n",
    "\n",
    "    def add_reference_transcription(self, reference, transcription):\n",
    "        self.add_reference(reference)\n",
    "        self.add_transcription(transcription)\n",
    "    \n",
    "    def length(self):\n",
    "        assert len(self.reference) == len(self.transcription)\n",
    "        return len(self.reference)\n",
    "\n",
    "    def print_reference_transcription(self, i):\n",
    "        if i >= 0 and i < len(self.reference):\n",
    "            print(\"Reference:   \\t: \", self.reference[i])\n",
    "            print(\"Transcription: \\t: \", self.transcription[i])\n",
    "\n",
    "    \n",
    "    def print_head(self):\n",
    "        self.print_reference_transcription(i=0)\n",
    "\n",
    "    def print_tail(self):\n",
    "        self.print_reference_transcription(i=self.length()-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_librispeech_data():\n",
    "    data = Data(\"librispeech\")\n",
    "    root_dir = \"LibriSpeech/test-clean/\"\n",
    "    model_dir = \"deepspeech\"\n",
    "\n",
    "    for filename in glob.iglob(root_dir + '**/*.trans.txt', recursive=True):\n",
    "        \n",
    "        file = open(filename)\n",
    "\n",
    "        for line in file.readlines():\n",
    "            idx = line.split()[0]\n",
    "            reference_text = \" \".join(line.split()[1:])\n",
    "\n",
    "            fid = \"/\".join(idx.split(\"-\")[:-1]) # idx to file id\n",
    "\n",
    "            fname = os.path.join(root_dir, fid, idx)\n",
    "            transcription_path = fname + \".\" + model_dir + \".transcription.txt\"\n",
    "            if os.path.exists(transcription_path):\n",
    "                transcription = helper.read_transcription(transcription_path)\n",
    "                data.add_reference_transcription(reference_text, transcription)\n",
    "            else:\n",
    "                raise ValueError(\"missing transcription: \" + transcription_path)\n",
    "\n",
    "        file.close()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  eleven oclock had struck it was a fine clear night they were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in their valley or on the banks of their river\n",
      "Transcription: \t:  eleven oclock had struck it was a fine clear night there were the only persons on the road and they sauntered leisurely along to avoid paying the price of fatigue for the recreation provided for the toledans in the valley or on the banks of their river\n",
      "\n",
      "Reference:   \t:  then the leader parted from the line\n",
      "Transcription: \t:  then the leader parted from the line\n"
     ]
    }
   ],
   "source": [
    "data = read_librispeech_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_fpath: str):\n",
    "    file = open(corpus_fpath)\n",
    "    corpus = file.readlines()\n",
    "    texts = []\n",
    "    for text in corpus:\n",
    "        texts.append(text[:-1])\n",
    "\n",
    "    return texts\n",
    "\n",
    "def read_crossasr_data():\n",
    "    data = Data(\"crossasr\")\n",
    "\n",
    "    transcription_dir = \"CrossASR/europarl-seed2021/data/transcription\"\n",
    "    tts_name = \"rv\"\n",
    "    asr_name = \"deepspeech\"\n",
    "    transcription_dir = os.path.join(transcription_dir, tts_name)\n",
    "    transcription_dir = os.path.join(transcription_dir, asr_name)\n",
    "\n",
    "    references = read_corpus(\"CrossASR/europarl-seed2021/corpus/europarl-20000.txt\")\n",
    "    \n",
    "    for i in range(len(references)):\n",
    "        transcription_path = os.path.join(transcription_dir, f\"{i+1}.txt\")\n",
    "        transcription = helper.read_transcription(transcription_path)\n",
    "\n",
    "        data.add_reference_transcription(references[i], transcription)\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:   \t:  in the european year for intercultural dialogue we should also recognize the importance of cultural industries in creating awareness and understanding of other cultures and therefore their importance for social cohesion\n",
      "Transcription: \t:  in the european year for inter cultural dialogue we should also recognize the importance of cultural industries in creating awareness and understanding of other cultures and therefore their importance for social cohesion\n",
      "\n",
      "Reference:   \t:  it must be organised by a single body responsible for ensuring that it is applied comprehensively consistently and effectively\n",
      "Transcription: \t:  it must be organized by a single body responsible for ensuring that it is applied comprehensively consistently and effectively\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "data.print_head()\n",
    "print()\n",
    "data.print_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asr_evaluation.asr_evaluation import asr_evaluation\n",
    "\n",
    "class Analyzer(object):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## used for caching\n",
    "        self.name = None\n",
    "        self.infos = []\n",
    "        self.word_count = []\n",
    "        self.word_accuracy = None\n",
    "        self.common_errors = None\n",
    "\n",
    "        \n",
    "    def analyze(self, data: Data):\n",
    "\n",
    "        ## get from cache if it is already computed before\n",
    "        if data.get_name() == self.name :\n",
    "            return self.infos, self.word_count\n",
    "        \n",
    "        infos = []\n",
    "        word_count = collections.Counter()\n",
    "        \n",
    "        for reference, transcription, in zip(data.get_reference(), data.get_transcription()) :\n",
    "            \n",
    "            ## create statistics for word counter\n",
    "            word_count += collections.Counter(reference.split())\n",
    "            \n",
    "            ## create statistics for errors\n",
    "            wer = jiwer.wer(reference, transcription)\n",
    "            if wer != 0:\n",
    "                evaluation = asr_evaluation.ASREvaluation()\n",
    "                evaluation.detect_word_error(reference, transcription)\n",
    "                confusion = evaluation.get_confusions()\n",
    "                infos.append(\n",
    "                    {\"confusion\": confusion, \"reference\": reference, \"transcription\": transcription})\n",
    "        \n",
    "        ## update the cache\n",
    "        self.name = data.get_name()\n",
    "        self.infos = infos\n",
    "        self.word_count = word_count\n",
    "        ## delete statistics\n",
    "        self.word_accuracy = None\n",
    "        self.common_errors = None\n",
    "\n",
    "        \n",
    "        return infos, word_count\n",
    "\n",
    "    def get_word_accuracy(self, data: Data):\n",
    "        \"\"\"Calculate word accuracy, which is the number of error (deletion or subsitution) divided by the number of word count\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## use caching if it is already computed before\n",
    "        if data.get_name() == self.name and self.word_accuracy != None:\n",
    "            return self.word_accuracy\n",
    "\n",
    "        \n",
    "        infos, word_count = self.analyze(data)\n",
    "        error_count = collections.Counter()\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "\n",
    "            ## get error from word substitution\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                \n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "                \n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "            ## get error from word deletion\n",
    "            if len(confusion[\"deletion\"]) > 0:\n",
    "\n",
    "                curr_error_count = {}\n",
    "                for i in range(len(confusion[\"deletion\"])):\n",
    "                    word_reference = confusion[\"deletion\"][i][\"word\"]\n",
    "                    count = confusion[\"deletion\"][i][\"count\"]\n",
    "\n",
    "                    curr_error_count[word_reference] = count\n",
    "\n",
    "                error_count += collections.Counter(curr_error_count)\n",
    "\n",
    "        \n",
    "        word_accuracy = {}\n",
    "        for word in word_count :\n",
    "            \n",
    "            ## if the word in the reference_text does not appear in the error word\n",
    "            ## then all the occurence of the word is correctly predicted \n",
    "            if word not in error_count :\n",
    "                word_accuracy[word] = 100.0 \n",
    "            else :\n",
    "                curr_word_accuracy = 100.0 - (100 * round(error_count[word]/word_count[word], 2))\n",
    "                assert curr_word_accuracy <= 100 and curr_word_accuracy >= 0\n",
    "                word_accuracy[word] = curr_word_accuracy\n",
    "\n",
    "        ## sort the word accuracy based on the value\n",
    "        word_accuracy = dict(sorted(word_accuracy.items(),\n",
    "                                   key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "        ## update cache\n",
    "        self.word_accuracy = word_accuracy\n",
    "        \n",
    "        return word_accuracy\n",
    "\n",
    "\n",
    "    def save_word_accuracy(self, word_accuracy, fpath):\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            keys = list(word_accuracy.keys())\n",
    "            values = list(word_accuracy.values())\n",
    "            file.write(f\"{'Word':15s} {'Accuracy'}\")\n",
    "            for i in range(len(keys)):\n",
    "                file.write(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "\n",
    "    def print_lowest_word_accuracy(self, word_accuracy, limit=10):\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(len(keys)-limit, len(keys)):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "    def print_highest_word_accuracy(self, word_accuracy, limit=10):\n",
    "        keys = list(word_accuracy.keys())\n",
    "        values = list(word_accuracy.values())\n",
    "        print(f\"{'Word':15s} {'Accuracy'}\")\n",
    "        for i in range(limit):\n",
    "            print(f\"{keys[i]:15s} {values[i]}\")\n",
    "\n",
    "\n",
    "    def get_most_common_errors(self, data: Data):\n",
    "        \n",
    "        infos, _ = self.analyze(data)\n",
    "        \n",
    "        ## geta result from caching if it is already computed before\n",
    "        if data.get_name() == self.name and self.common_errors != None:\n",
    "            return self.common_errors\n",
    "\n",
    "        ## TODO: use Counter library\n",
    "        common_errors = {}\n",
    "        for info in infos:\n",
    "            confusion = info[\"confusion\"]\n",
    "            if len(confusion[\"substitution\"]) > 0:\n",
    "                for i in range(len(confusion[\"substitution\"])):\n",
    "                    word_reference = confusion[\"substitution\"][i][\"word_reference\"]\n",
    "                    word_substitution = confusion[\"substitution\"][i][\"word_substitution\"]\n",
    "                    count = confusion[\"substitution\"][i][\"count\"]\n",
    "                    if word_reference in common_errors:\n",
    "                        substitutions = common_errors[word_reference]\n",
    "                        if word_substitution in substitutions:\n",
    "                            common_errors[word_reference][word_substitution] = count + \\\n",
    "                                common_errors[word_reference][word_substitution]\n",
    "                        else:\n",
    "                            common_errors[word_reference][word_substitution] = count\n",
    "                    else:\n",
    "                        common_errors[word_reference] = {\n",
    "                            word_substitution: count}\n",
    "\n",
    "        ## TODO : put this in a separate function dict_sorter\n",
    "        ## sort things inside a substitution error\n",
    "        for key in common_errors.keys():\n",
    "            common_errors[key] = dict(sorted(common_errors[key].items(),\n",
    "                                            key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        ## sort words based on the highest occurence\n",
    "        common_errors = dict(sorted(common_errors.items(),\n",
    "                                        key=lambda item: list(item[1].values())[0], reverse=True))\n",
    "        \n",
    "        self.common_errors = common_errors\n",
    "        \n",
    "        return common_errors\n",
    "\n",
    "    def print_common_error(self, common_errors, limit=2):\n",
    "        count = 0\n",
    "        print_limit = 16\n",
    "        for word, common in common_errors.items():\n",
    "            if count < print_limit :\n",
    "                print(\"Word: \", word)\n",
    "                # print(\"Substituion: \")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(min(limit, len(keys))):\n",
    "                    print(f\"\\t{keys[i]:10s} count: {values[i]}\")\n",
    "            count += 1\n",
    "\n",
    "    def save_common_errors(self, common_errors, fpath):\n",
    "        os.makedirs(pathlib.Path(fpath).parent.absolute(), exist_ok=True)\n",
    "        with open(fpath, 'w') as file:\n",
    "            for word, common in common_errors.items():\n",
    "                file.write(f\"Word: {word}\\n\")\n",
    "                keys = list(common.keys())\n",
    "                values = list(common.values())\n",
    "                for i in range(len(keys)):\n",
    "                    file.write(f\"\\t{keys[i]:10s} count: {values[i]}\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "mo              0.0\n",
      "schooled        0.0\n",
      "iridescent      0.0\n",
      "toothed         0.0\n",
      "teal            0.0\n",
      "frequenter      0.0\n",
      "clutching       0.0\n",
      "talons          0.0\n",
      "sank            0.0\n",
      "feeding         0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "oclock          100.0\n",
      "struck          100.0\n",
      "fine            100.0\n",
      "clear           100.0\n",
      "only            100.0\n",
      "persons         100.0\n",
      "road            100.0\n",
      "sauntered       100.0\n",
      "leisurely       100.0\n",
      "along           100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = Analyzer()\n",
    "\n",
    "data = read_librispeech_data()\n",
    "\n",
    "word_accuracy = analyzer.get_word_accuracy(data)\n",
    "fpath = \"output/librispeech/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(word_accuracy, fpath)\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(word_accuracy)\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(word_accuracy)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  in\n",
      "\tand        count: 44\n",
      "\tan         count: 3\n",
      "Word:  a\n",
      "\tthe        count: 34\n",
      "\tof         count: 4\n",
      "Word:  and\n",
      "\tin         count: 28\n",
      "\ta          count: 3\n",
      "Word:  this\n",
      "\tthe        count: 21\n",
      "\tspilling   count: 1\n",
      "Word:  an\n",
      "\tand        count: 13\n",
      "\ton         count: 1\n",
      "Word:  too\n",
      "\tto         count: 10\n",
      "\ttwo        count: 3\n",
      "Word:  two\n",
      "\tto         count: 8\n",
      "\tlotto      count: 1\n",
      "Word:  is\n",
      "\tas         count: 7\n",
      "\this        count: 4\n",
      "Word:  the\n",
      "\ta          count: 7\n",
      "\tto         count: 2\n",
      "Word:  uncas\n",
      "\tonce       count: 6\n",
      "\tone        count: 1\n",
      "Word:  thee\n",
      "\tthe        count: 6\n",
      "\the         count: 4\n",
      "Word:  of\n",
      "\ta          count: 6\n",
      "\tat         count: 2\n",
      "Word:  anyone\n",
      "\tone        count: 6\n",
      "Word:  men\n",
      "\tman        count: 6\n",
      "\tthen       count: 1\n",
      "Word:  boolooroo\n",
      "\tbolero     count: 6\n",
      "\tbooleroo   count: 4\n",
      "Word:  has\n",
      "\thad        count: 5\n",
      "\tas         count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/librispeech/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lowest Accuracy Rate\n",
      "Word            Accuracy\n",
      "ayuso           0.0\n",
      "overexploited   0.0\n",
      "oft             0.0\n",
      "scams           0.0\n",
      "maroni          0.0\n",
      "formers         0.0\n",
      "cafÃ©            0.0\n",
      "shah            0.0\n",
      "masood          0.0\n",
      "tunisias        0.0\n",
      "\n",
      "=== Highest Accuracy Rate\n",
      "Word            Accuracy\n",
      "dialogue        100.0\n",
      "should          100.0\n",
      "cultural        100.0\n",
      "creating        100.0\n",
      "understanding   100.0\n",
      "other           100.0\n",
      "cultures        100.0\n",
      "therefore       100.0\n",
      "union           100.0\n",
      "international   100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_crossasr_data()\n",
    "\n",
    "analyzer = Analyzer()\n",
    "word_accuracy = analyzer.get_word_accuracy(data)\n",
    "fpath = \"output/crossasr/word_accuracy.txt\"\n",
    "analyzer.save_word_accuracy(word_accuracy, fpath)\n",
    "\n",
    "\n",
    "print(\"=== Lowest Accuracy Rate\")\n",
    "analyzer.print_lowest_word_accuracy(word_accuracy)\n",
    "print()\n",
    "\n",
    "print(\"=== Highest Accuracy Rate\")\n",
    "analyzer.print_highest_word_accuracy(word_accuracy)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Most common errors\n",
      "Word:  and\n",
      "\tin         count: 1842\n",
      "\tan         count: 154\n",
      "Word:  today\n",
      "\tday        count: 336\n",
      "\tto         count: 8\n",
      "Word:  is\n",
      "\tas         count: 281\n",
      "\this        count: 42\n",
      "Word:  a\n",
      "\tthe        count: 281\n",
      "\tof         count: 80\n",
      "Word:  has\n",
      "\thad        count: 265\n",
      "\thave       count: 8\n",
      "Word:  favour\n",
      "\tfavor      count: 207\n",
      "\tto         count: 1\n",
      "Word:  rapporteur\n",
      "\treporter   count: 183\n",
      "\trapier     count: 16\n",
      "Word:  cooperation\n",
      "\toperation  count: 163\n",
      "\tation      count: 1\n",
      "Word:  member\n",
      "\tmembers    count: 149\n",
      "\tember      count: 2\n",
      "Word:  programmes\n",
      "\tprograms   count: 145\n",
      "\tprogram    count: 7\n",
      "Word:  s\n",
      "\tas         count: 144\n",
      "\tus         count: 67\n",
      "Word:  as\n",
      "\tis         count: 140\n",
      "\this        count: 13\n",
      "Word:  thank\n",
      "\tthink      count: 138\n",
      "\tevery      count: 2\n",
      "Word:  ensure\n",
      "\tinsure     count: 128\n",
      "\tinsured    count: 8\n",
      "Word:  mr\n",
      "\tr          count: 123\n",
      "\tm          count: 37\n",
      "Word:  eu\n",
      "\te          count: 120\n",
      "\tu          count: 74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Most common errors\")\n",
    "\n",
    "common_errors = analyzer.get_most_common_errors(data)\n",
    "fpath = \"output/crossasr/common_errors.txt\"\n",
    "analyzer.save_common_errors(common_errors, fpath)\n",
    "\n",
    "analyzer.print_common_error(common_errors)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
